<html>
<head>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
    <title>Xinpeng Chen</title>
    <meta content="Xinpeng Chen, chenxinpeng.github.io" name="keywords" />
    <style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
        border: 0pt none;
        font-family: inherit;
        font-size: 100%;
        font-style: inherit;
        font-weight: inherit;
        margin: 0pt;
        outline-color: invert;
        outline-style: none;
        outline-width: 0pt;
        padding: 0pt;
        vertical-align: baseline;
    }
    a {
        color: #1772d0;
        text-decoration:none;
    }
    a:focus, a:hover {
        color: #f09228;
        text-decoration:none;
    }
    a.paper {
        font-weight: bold;
        font-size: 12pt;
    }
    b.paper {
        font-weight: bold;
        font-size: 12pt;
    }
    * {
        margin: 0pt;
        padding: 0pt;
    }
    body {
        position: relative;
        margin: 3em auto 2em auto;
        width: 900px;
        font-family: Lato, Verdana, Helvetica, sans-serif;
        font-size: 14px;
        background: #eee;
    }
    h2 {
        font-family: Lato, Verdana, Helvetica, sans-serif;
        font-size: 14pt;
        font-weight: 600;
    }
    h3 {
        font-family: Lato, Verdana, Helvetica, sans-serif;
        font-size: 12px;
        font-weight: 600;
    }
    strong {
        font-family: Lato, Verdana, Helvetica, sans-serif;
        font-size: 13px;
        font-weight:bold;
    }
    ul {
        list-style: circle;
    }
    img {
        border: none;
    }
    li {
        padding-bottom: 0.5em;
        margin-left: 1.4em;
    }
    alert {
        font-family: Lato, Verdana, Helvetica, sans-serif;
        font-size: 13px;
        font-weight: bold;
        color: #FF0000;
    }
    em, i {
        font-style:italic;
    }
    div.section {
        clear: both;
        margin-bottom: 1.5em;
        background: #eee;
    }
    div.spanner {
        clear: both;
    }
    div.paper {
        clear: both;
        margin-top: 0.5em;
        margin-bottom: 1em;
        border: 1px solid #ddd;
        background: #fff;
        padding: 1em 1em 1em 1em;
    }
    div.paper div {
        padding-left: 230px;
    }
    img.paper {
        margin-bottom: 0.5em;
        float: left;
        width: 200px;
    }
    span.blurb {
        font-style:italic;
        display:block;
        margin-top:0.75em;
        margin-bottom:0.5em;
    }
    pre, code {
        font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
        margin: 1em 0;
        padding: 0;
    }
    div.paper pre {
        font-size: 0.9em;
    }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
</script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)}, i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-66888300-1', 'auto');
    ga('send', 'pageview');
</script>

<body>
    <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
        <div style="margin: 0 auto; width: 100%;">
            <img title="Xinpeng Chen" style="float: left; padding-left: .01em; height: 140px;" src="assets/images/xinpeng_chen_2018.png" />
            <div style="padding-left: 18em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 18pt;">Xinpeng Chen (<a href="publication/CV_Xinpeng_Chen.pdf">CV</a>)</span><br />
                <span><strong>Email  </strong>: jschenxinpeng [AT] gmail [DOT] com</span><br/>
                <span>I am a research intern at the <a href="http://ai.tencent.com/ailab/index.html">Tencent AI Lab</a> and mainly supervised by <a href="http://forestlinma.com/">Lin Ma</a> .</span>
            </div>
        </div>
    </div>
    <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

    <div style="clear: both;">
        <div class="section">
            <h2>Research Interests</h2>
            <div class="paper">
                My research interests lie in the area of deep learning and multimodal learning, specifically for image and language.
                Currently, I mainly focus on referring expression comprehension.
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2>Work Experience</h2>
            <div class="paper">
                Nov. 2016 - Feb. 2017, Research intern, NLP Group, <a href="http://www.hitachi.com/rd/index.html">Hitachi Central Research Laboratory</a>, Tokyo
                <br> Mentor: <a href="https://scholar.google.com/citations?user=LC_x1w4AAAAJ&hl=en">Dr. Bin Tong</a>
                <br> Topic: Video captioning; Image paragraph description; Reinforcement learning for image captioning.
                <br/><br/>
                Mar. 2017 - Nov. 2018, Research intern, CV Group, <a href="http://ai.tencent.com/ailab/index.html">Tencent AI Lab</a>, Shenzhen
                <br> Mentor: <a href="http://forestlinma.com/">Dr. Lin Ma</a>, <a href="https://dblp.org/pers/j/Jiang:Wenhao">Dr. Wenhao Jiang</a>, and <a href="http://www.ee.columbia.edu/~wliu/">Dr. Wei Liu</a>
                <br> Topic: Image and video captioning; Large video classification; Referring expression comprehension.
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2>Publications</h2>
            <div class="paper" id="EMNLP2018">
                <img class="paper" src="assets/images/emnlp_2018_tgns.png" title="Temporally Grounding Natural Sentence in Video" />
                <div> <strong>Temporally Grounding Natural Sentence in Video</strong><br />
                    Jingyuan Chen, <strong>Xinpeng Chen</strong>, Lin Ma, Zequn Jie, Tat-Seng Chua <br/>
                    Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), Brussels, Belgium, 2018.<br/>
                    <a href='http://www.aclweb.org/anthology/D18-1015'>[PDF]</a>
                </div>
                <div class="spanner"></div>
            </div>

            <div class="paper" id="CVPR2018">
                <img class="paper" src="assets/images/ARNet.png" title="Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present"/>
                <div> <strong>Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present</strong><br />
                    <strong>Xinpeng Chen</strong>, Lin Ma, Wenhao Jiang, Jian Yao, Wei Liu <br/>
                    IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Salt Lake City, USA, 2018.<br/>
                    <a href='publication/ARNet_poster.pdf'>[PDF]</a>
                    <a href='publication/ARNet_supplementary_material.pdf'>[Supplementary Material]</a>
                    <a href='https://github.com/chenxinpeng/ARNet'>[Code]</a>
                    <a href="publication/ARNet_poster.pdf">[Poster]</a>
                    <a href="https://www.youtube.com/watch?v=qmNtd9JY04s">[Video]</a>
                </div>
                <div class="spanner"></div>
            </div>

            <div class="paper" id="WWW2018">
                <img class="paper" src="assets/images/www2018.png" title="Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-World Dataset" />
                <div> <strong>Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-World Dataset</strong><br />
                    <strong>Xinpeng Chen</strong>, Jingyuan Chen, Lin Ma, Jian Yao, Wei Liu, Jiebo Luo, Tong Zhang <br/>
                    International World Wide Web Conference (<strong>WWW</strong>), The Big Web Track, Lyon, France, 2018. <br/>
                    <a href='publication/FVAD.pdf'>[PDF]</a>
                </div>
                <div class="spanner"></div>
            </div>

            <div class="paper" id="AAAI2018">
                <img class="paper" src="assets/images/aaai2018.png" title="Learning to Guide Decoding for Image Captioning" />
                <div> <strong>Learning to Guide Decoding for Image Captioning</strong><br />
                    Wenhao Jiang, Lin Ma, <strong>Xinpeng Chen</strong>, Fumin Shen, Hanwang Zhang, and Wei Liu <br/>
                    The AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), Louisiana, USA, 2018. <br/>
                    <a href='publication/ltg.pdf'>[PDF]</a>
                </div>
                <div class="spanner"></div>
            </div>

            <div class="paper" id="CVPR2017">
                <img class="paper" src="assets/images/youtube8m.png" title="Aggregating Frame-level Features for Large-Scale Video Classification" />
                <div> <strong>Aggregating Frame-level Features for Large-Scale Video Classification</strong><br />
                    Shaoxiang Chen, Xi Wang, Yongyi Tang, <strong>Xinpeng Chen</strong>, Zuxuan Wu, Yugang Jiang <br/>
                    YouTube-8M Large-Scale Video Understanding Challenge (<strong>CVPR Workshop</strong>), Hawaii, USA, 2017. <br/>
                    <a href='publication/youtube8m.pdf'>[PDF]</a>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2>Professional Activities</h2>
            <div class="paper">
                <ul>
                    <li>Reviewer for AAAI (2019).</li>
                </ul>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2>Miscellaneous</h2>
            <div class="paper">
                <ul>
                    <li>
                        Master Thesis: Research on Image Semantic Caption Generation Based on Encoder-Decoder Framework.  <a href="publication/master_thesis.pdf">[PDF]</a>
                    </li>
                </ul>
                <ul>
                    <li>
                        Tensorflow implement of paper: A Hierarchical Approach for Generating Descriptive Image Paragraphs.  <a href="https://github.com/chenxinpeng/im2p">[Code]</a>
                    </li>
                </ul>
                <ul>
                    <li>
                        Tensorflow implement of paper: Sequence to Sequence - Video to Text.  <a href="https://github.com/chenxinpeng/S2VT">[Code]</a>
                    </li>
                </ul>
                <ul>
                    <li>
                        Tensorflow implement of paper: Optimization of image description metrics using policy gradient methods.  <a href="https://github.com/chenxinpeng/Optimization_of_image_description_metrics_using_policy_gradient_methods">[Code]</a>
                    </li>
                </ul>
                <ul>
                    <li>
                        Detecting the text in natural images by SSD (Single Shot Detection).  <a href="https://github.com/chenxinpeng/SSD_scene_text_detection">[Code]</a>
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <p align="right"><font size="5">Last Updated on 4-th Nov., 2018</a></font></p>
        <p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
    </div>
</body>
</html>