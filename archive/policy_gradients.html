<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Fine-grained Video Attractiveness Prediction</title>
    <link rel="stylesheet" href="../../assets/css/jemdoc.css", type="text/css" />
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>
<body>
<div id="layout-content">
    <div id="toptitle">
        <h1 id="fvad">什么叫做 Policy Gradients?</h1>
    </div>
    所谓的 Policy Gradients。不妨先说回 Supervised Learning，一个模型，有一些参数，这时候输入进一张图像。
    对于分类问题，我们得到所有类别的概率（事实上，我们用的是 log 后的概率，因为 log 函数的单调递增性）。同时，我们还有这张图像正确的 label。
    于是，我们便可以通过计算梯度，反应到网络模型的参数上，来增加这张图像输出正确标签的概率。
    <br><br>
    但是对于 Reinforcement Learning 而言，我们是没有正确标签的。
    我们有的是环境（Environment）对于模型输出的反馈，也就是所谓的奖赏（Reward）。
    这时候，输入图像，计算出所有类别的概率之后，模型会采样出一个动作（Action）。
    我们根据环境对于模型的奖赏，来更新模型的参数，增加奖赏多的采样动作的概率，减少奖赏少的动作的采样的概率。
</div>
</body>
</html>